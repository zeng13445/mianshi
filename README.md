# 记录一下面试的笔记
- java
1. HashMap：
    - 阐述
    ```
    底层主要是基于数组外加链表组成（1.8加入红黑树），链表主要是为了解决hash冲突，还可解决数组插入数据效率低的问题。
    ```
    - 默认容量和扩容机制
    ```
    默认capacity 16，默认负载因子0.75，所有当大于12时才发生扩容（一倍）。扩容是很消耗性能的，所以最好提前预估初始大小以减少扩容带来的性能损耗。
    ```
    - put、get（1.8）
    ```
    根据key计算出hashcode，然后定位出所在的桶（当前位置），再将value放入。get类似，也是通过key算出hashcode，再定位到桶，取值。
    ```
     ![image](https://github.com/zeng13445/mianshi/blob/main/images/put.png)
    - 初始化
    ```
    若在构造函数中指定了容量大小7，那么该容量其实是8，因为hash会选择大于该数字的第一个2的n次幂作为容量（即3->4、7->8、9->16）。
    ```
2. ConcurrentHashMap：
    - 阐述
    ```
    其实就是同步的HashMap，底层数据结构依旧是数组+链表+红黑树（链表节点大于8转为红黑树）。在多线程场景下解决了hashmap死循环的问题。
    线程安全方式采用CAS + synchronized方式，又解决了hashtable在多线程场景下性能问题。
    ```
    - hashmap、hashtable、concurrenthashmap
    ![image](https://github.com/zeng13445/mianshi/blob/main/images/difference.png)
    
3. GC
    - gc作用
    ```
    gc垃圾回收机制，在适当的时候回收JVM垃圾，防止内存泄露，有效的使用可以使用的内存。
    ```
    - 什么时候触发
    ```
    ......
    ```
    - 对谁
    ```
    不再使用的对象。使用"引用计数法"、"可达性分析算法"判断对象是否还在被使用。
    引用计数法：给对象添加一个引用计数器，被引用时+1，引用失效-1。很难解决对象之间的互相循环引用（引用环问题）的问题。
    可达性分析算法（主流）：算法的基本思路就是以一系列的称为“GC Roots”的对象作为起点，从这些节点开始向下搜索，搜索所走过的路径称为引用链，
    当一个对象到GC Roots没有任何引用链相连的时候（即该对象不可达），则证明此对象是不可用的。在java中，可作为GC Roots的对象包括以下几种：
    栈中引用的对象（栈帧中的本地变量表）、方法区中类静态属性引用的对象、方法区中常量引用的对象。
    ```
    - 做了什么
    ```
    不可达的对象，如何被回收：1.标记清除法，2.复制算法，3.标记整理法，4.分代收集算法
    标记清除法：在标记（可达性算法标记）完成后统一回收所有被标记的对象。
    复制算法：将可用内存分为大小相同的两部分，每次只使用其中的一块，当使用的那一块内存快用尽时，就将还存活的对象复制到另外一块内存上，然后把已经使用过的内存空间一次性清理掉。
    标记整理法：标记过程不变，仍使用“可达性分析算法”，标记完后不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。
    分代收集算法：JVM在实际垃圾回收中实际使用的是分代收集算法：根据对象存活周期的不同将内存划分为：新生代和老年代。在新生代每次都只有少量对象存活，选用复制算法；
    老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用标记-整理法或是标记-清理法进行回收。
    ```
[TOC]

# 简历技术栈

## 1.请你说下Java8新特性

    1.lambda表达式
    2.stream流
    3.optional类
    4.时间日期LocalDateTime、LocalDate
    5.字符串拼接StringJoin
    6.函数式接口 接口上加了@FunctionalInterface 有且仅有一个抽象方法的接口。

## 2.你有用到什么设计模式吗？

```markdown
1.利用模版方法模式+工厂模式重构创建订单接口
    

原因：在之前的时候每多一种订单类型，代码有需要多一个
if_else,他们可能只是计算规则不一样，以及获取的
折扣优惠不一样，但是每次加一个订单都需要改原来
的代码，不利于扩展和维护，甚至会出问题。

解决：创建一个抽象的订单公共类，把所有订单的共有逻辑写成模版固定好
然后将他们不同的方法抽取成抽象的方法，比如计算定的优惠、获取订单的
折扣抽取成两个抽象的接口，然后然相应的订单类型去实现，比如vip订单
就集成这个公共的模版实现他的优惠计算和折扣计算，普通订单一样
	然后再利用工厂模式提供一个根据订单类型创建订单的接口，对外部提
供工厂接口，不暴露我们底层的实现，这样对扩展开发，修改关闭了

2.观察者模式
概念：它用于解耦观察者和被观察者对象，使得它们之间可以独立地进行交互。被观察者发生变动的时候，所有的
观察者都会发生变动
```

## 3.说一下你对Spring的理解？

    	Spring是分层的 Java 一站式 轻量级开源框架，他有两大核心，也就是spring的IOC和AOP,
    springIOC是控制反转，也就是创建对象的权力交给了spring,以后用对象我们只需要到spring
    容器中通过Autowired拿到就行，他降低了降低类与类之间的耦合，底层是通过工厂模式+配置文件+反射实现的。
    工厂模式  创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。
    三种注入方式 构造函数注入、setter方法注入、普通属性的注入（spring如何解决循环依赖的）


    	SpringAOP则是面向切面编程，将程序中的业务抽取成不同的切面，重新组装各个业务，减低业务逻辑之间的耦合，
    我们可以在方法前后做一些增强，底层通过动态代理实现对目标对象的增强，spring在选择代理方式的时候，如果目
    标类实现了接口，默认会采用jdk动态代理，如果目标类没实现接口，默认采用CGlib动态代理。
    	区别：
    	实现方式：
    		JDK动态代理是通过Java的反射机制实现的，而CGLIB动态代理则是通过生成目标类的子类来实现的。
    	需要的条件：JDK动态代理要求目标类必须实现接口，因为代理对象需要实现与目标对象相同的接口；而CGLIB动态代理则
    		可以代理没有实现接口的目标类。
    	性能：JDK动态代理在性能上通常比CGLIB动态代理稍好，因为JDK动态代理是基于接口的，而接口在Java的字节码中是
    		一种轻量级的结构。而CGLIB动态代理生成的子类则是目标类的子类，会继承目标类的所有方法，包括final方法，
    		这可能会导致性能略有下降。
    AspectJ：
    基本语法：
    切面：切点+通知
    切点：指向要增强的方法
    通知：具体增强的操作（前置通知、后置通知、异常通知、最终通知、环绕通知）
    
    面试题：	
    	1.SpringBoot的自动装配原理：
    		（1）SpringBoot 自动装配主要是基于注解编程 和约定优于配 置的思想来设计的
    		（2）约定大于配置的思想：把要加载的配置文件放在META-INF\spring.fatories文件中
    		（3）流程：
    			@SpringBootApplication组合注解 ==> @SpringbootConfiguration（配置注解）、
    			EnableAutoConfiguration（扫描配置类）、@ComponentScan(扫描spring注解) 
    		EnableAutoConfiguration自动配置类里面，会注入一个AutoConfigurationImportSelector选择器，
    		可以帮助将所有符合条件的@Configuration配置交给spring的IOC容器进行注入，里面有一个selectImports
    		方法，他会去加载（META-INF\spring.fatories文件），最后通过SpringFactoriesLoader机制创建对应bean
    		并注入到spring容器当中
    	
    	2.Spring依赖注入问题解决
    		（1）概念：在Spring中，循环依赖指的是两个或多个bean之间相互依赖，形成了一个环形依赖关系。
    		（2）常见的解决方式有三种：
    				- 构造函数注入
    				- setter方法注入
    				- 使用代理对象
    	3.二级缓存能能解决循环依赖吗，为啥要用三级缓存解决？
    		（1）二级缓存不能解决循环依赖：在 Spring 中，二级缓存是指缓存单个 BeanFactory 中的 Bean 实例，
    			它的作用是优化 bean 的创建和销毁过程，提高系统的性能。而三级缓存则是在二级缓存的基础上进行了优化，
    			通过代理对象解决了循环依赖的问题。
    		（2）三级缓存引入了代理对象，通过在代理对象中解决循环依赖的问题，避免了 bean 实例的不完整依赖注入。
    			具体来说，当创建 bean 的过程中出现循环依赖时，Spring 会先将 bean 放入三级缓存中，然后使用代
    			理对象代替原始的 bean，以完成依赖注入。代理对象中保存了当前 bean 的创建状态，当需要使用该 bean 
    			时，Spring 会从二级缓存中获取代理对象，从而避免了循环依赖问题。
    
    	4.Springbean

## 4.说一下你对Synchronized的理解？

    1.它是什么
    	Synchronized它是一种悲观锁，被它修饰的方法或者代码块同一时刻最多只有一个线程执行
    
    2.他是如何使用的
    	作用于实例方法
    		【对象锁】作用于实例方法，当前实例加锁，进入同步代码前要获得当前实例的锁
    	作用于代码块
    		【对象锁】作用于代码块，对括号里配置的对象加锁
    	作用于静态方法
    		【类锁】作用于静态方法，当前类加锁，进去同步代码前要获得当前类对象的锁
    
    3.根据字节码分析
    		作用于同步代码块，底层是通过monitorenter进行加锁，monitorexit释放锁的（一般有两个monitorexit，
    	保证正常情况可以退出锁的持有，异常情况也可以退出所得持有）
    		作用于方法，则是通过flags标识判断是普通同步方法还是静态同步方法，最后再用monitor进行加锁和释放锁的
    
    4.它的底层实现
    		它底层是管程monitor实现的，管程：又叫监视器，是一种程序结构，他把信号量以及操作原语封装在一个对象内部保证
    	互斥访问资源，保证了在一个时间点中只有持有了管程的线程才能访问方法或者资源（也就是我们常说的锁）
    	每个对象里面都又一个对象监视器(monitor管程)，里面的属性标识了哪个线程持有了这个对象（_owner）
    
    5.对象在堆中的布局
        对象头
            对象标记MarkWord
                【存放】：GC的信息、哈希码的存储、同步锁的标记、偏向锁的持有
            类元信息（又叫类型指针）
                虚拟机通过这个指针来确定这个对象是哪个类的实例
            对象头有多大？
                在64位系统中，MarkWord占了8个字节，类型指针占了8个字节（忽略压缩指针的影响），一个对象初始化大概16个字节
        实例数据
            存放类的属性数据信息，包括父类的属性信息
        对齐填充
            虚拟机规定对象的起始地址是8个字节的整数倍
    
    6.synchronized锁升级
    	背景：java5以前这个是操作系统级别的重量级操作（锁竞争激烈性能会急剧下降），加锁会阻塞其他线程，其他现线程会
    		在阻塞队列里面等待进行唤醒，阻塞、唤醒一个线程会进行用户态和内核态的切换，比较损耗性能java1.6之后为了减
    		少用户态和内核态的切换，减少获得锁和释放锁带来的性能消耗，就让引入了偏向锁和轻量级锁
    
    	本质：根据对象头里面的MarkWord的锁标记位以及是否偏向标记位的变化来进行锁升级的过程
    
    	过程：开始没有线程的时候是无锁，当有一个线程进来时锁升级为偏向锁，markword里面的记录当前线程的Id(偏向该线程)，
    		如果不存在竞争也就是在下一个线程在该线程安全点结束后进来，当前锁仍然是偏向锁,，markword里面的记录为当前线
    		程，如果存在竞争（第二个线程来的时候仍有字节码执行，也就是还没到安全点），锁从偏向锁升级为轻量级锁，后面来
    		的线程进行CAS自旋来获取锁；随着竞争越来越剧烈，CPU空转较重就会升级成重量级锁，除了当前线程其他线程都会阻塞
    		等待唤醒抢占锁，性能比较低，因为这个时候涉及到用户态到内核态的转换
    
    7.如果部署多台机器的话利用synchronized仍然会锁不住，依旧存在多个线程同时操作一个变量的情况，这种情况就要考虑使用
    	分布式锁了

## 5.说一下你对Volitile的理解？

```
（1）说到volitile那就不得不说下JMM内存模型，JMM内存模型有三大特征：可见性、原子性、有序性
（2）而被volitile修饰的变量具有可见性和有序性；
		可见性表现在：被volitile修饰的变量
					- 在进行写操作的时候，会立即刷新回主内存
					- 读操作的时候，其他线程的工作内存的数据会被清除，其他线程需要重新去主内存进行读取
（3）volitile的有序性保证了指令重排，底层是通过内存屏障实现的
		内存屏障︰是一种屏障指令，它使得CPU或编译器对屏障指令的前和后所发出的内存操作﹐执行一个排序的约束。
	也叫内存栅栏或栅栏指令【是一种屏障指令使得cpu对屏障前后的内存操作进行一个排序的约束】
	一般细分有四种：读读屏障、读写屏障、写写屏障、写读屏障
	写数据时加入屏障，强制将线程私有工作内存的数据刷回主物理内存
	读数据时加入屏障，线程私有工作内存的数据失效，重新到主物理内存中获取最新数据
	
（4）为什么volitile不能保证原子性？
	原因：根据volitile特性，在修改的变量在修改的时候回将修改的数据直接刷新回主内存，多并发的情况下，
		其中一个线程完成了操作（i++）,其他线程去读最新数据的时候，自己的工作内存的数据会被清除，也就
		是i++的操作失效了，所以操作数据的时候还是要加锁避免这种失效问题，所以volatile不能保证原子性！

```

## 6.说下你对AQS的理解？

```
AQS作为抽象队列同步器，它是由一个volitile修饰的共享变量state变量和FIFO的双端队列组成，
他作为抽象的其实解决了两个问题，一、锁的使用，通过state变量谁先抢到了state标志位1，二
锁的分配，抢的到锁的就使用，抢不到的在CLH双向队列里面排队


并发工具包下很多都是基于AQS来实现的比如Semaphore、CountDownLatch、ReentrantLock

```

## 7.SpringCloud五大组件基本使用？

    1.Eureka 注册中心：服务的注册与发现（服务较多不方便管理，提高效率）
    2.Ribbon 负载均衡组件，eureka、feign、zuul都已经集成了Ribbon  （负载均衡默认是轮询）
    3.Zuul、Gateway 网关组件  网关来实现 鉴权、动态路由等等操作。可以说网关是我们服务的统一入口
    4.Feign 远程调用组件，集成ribbon和hystrix  可以实现服务与服务之间的调用
    5.Hystrix 容错组件，用于隔离访问远程服务、第三方库，防止出现级联失败，及时处理服务异常
    主要手段 a.线程隔离 b.服务熔断 优先保证核心服务，而非核心服务不可用或弱可用。

## 8.SpringCloudAlibaba你了解那些？

    SpringCloudAlibaba分布式微服务框架是阿里巴巴提供，致力于提供微服务开发的一站式解决方案。
    它常见的组件：
    Nacos:注册中心负责服务的注册与发现，配置中心可以动态配置管理（服务的热部署）
    Ribbon:负载均衡组件
    Feign:远程服务调用组件
    Sentinel:容错组件，用于隔离访问远程服务、第三方库，防止出现级联失败，及时处理服务异常 ，服务容错、降级、熔断
    Gateway:网关组件，实现鉴权、动态路由等等操作。可以说网关是我们服务的统一入口
    Sleuth:调用链监控组件
    Seata：分布式事务解决组件
    
    使用Seata解决分布式事务问题：
    	1.什么事seata? 
    		它是阿里开源的一站式分布式事务解决方案，他提供了高性能，而且简单易用的分布式事务服务
    	2.Seata 主要包括三个基本组件：
    		Transaction Coordinator (TC)：事务协调器，负责管理全局事务的生命周期，包括全局事务的开始、提交和回滚。
    		Transaction Manager (TM)：事务管理器，负责定义全局事务的边界，向 TC 发起全局事务的开始、提交和回滚请求。
    		Resource Manager (RM)：资源管理器，负责管理分布式事务中参与者的资源（如数据库连接），与 TC 协同完成分支事务的提交			或回滚。
    	3.Seata 的基本工作流程如下：
    		TM 向 TC 发起全局事务的开始请求。
    		TM 向参与全局事务的各个 RM 发送注册分支事务请求。
    		RM 在本地执行分支事务，并将执行结果汇报给 TC。
    		TM 根据所有分支事务的执行结果，向 TC 发起全局事务的提交或回滚请求。
    		TC 协调各个 RM，执行全局事务的提交或回滚操作。
    	大概来说：两个阶段 RM是各个服务的分支事务，标记个各个服务的事务状态，RM会把状态提交给TM，TM再汇总信息判断是否提交给TC还是回滚
    	4.如何使用
    		1.导入Seata的依赖
    		2.配置数据源代理：将原有的数据源替换为 Seata 提供的数据源代理。
    		3.使用@GlobalTransactional标记需要控制分布式事务的方法上
    	5.涉及到的表
    	（1）Global_Table
    		Global_Table用于记录全局事务的信息，包括全局事务ID、事务状态、事务类型等。
    
    	（2）Branch_Table
    		Branch_Table用于记录分支事务的信息，包括分支事务ID、全局事务ID、分支事务状态、分支事务类型等。
    
    	（3）Lock_Table
    		Lock_Table用于记录分布式事务中所涉及到的业务数据的锁定状态，以防止在分布式事务过程中出现冲突。
    
    	（4）Undo_Log
    		Undo_Log表用于记录每个事务中涉及到的业务数据的修改前的值，以及用于回滚的SQL语句。当一个事务需要回滚时，
    		Seata会根据	Undo_Log表中记录的信息生成回滚SQL语句，然后将该SQL语句发送给相关的RM执行回滚操作，以实
    		现数据的一致性和完整性。
    
    	总结：Lock_Table和Undo_Log表中的数据通常是不再需要的，可以进行清理和删除。在Seata中，可以通过配置文件中的参数来指定
    		Lock_Table和Undo_Log表的过期时间和清理策略，以便及时清理这些数据，以减少存储和管理成本。
    		Global_Table和Branch_Table中的数据仍然需要保留，以便在需要时进行查询和统计。
    		在实际应用中，可以通过定期备份和归档Global_Table和Branch_Table表中的数据，以实现数据的安全管理和可持续发展。

## 9.对于Dubbo框架你了解那些？

    	Dubbo是一款高性能的Java RPC框架，它由阿里巴巴公司开发并开源。Dubbo可以在不同的进程之间进行远程过程调用，
    以实现微服务架构中服务的注册、发现、调用和管理。Dubbo的主要目标是提供高性能和透明化的远程调用能力，同时简化分布式服务之间的通信和管理。
    	Dubbo可以解决分布式系统中的服务治理和服务调用问题。它提供了服务注册、发现、负载均衡、故障转移、并发控制、服务降级、路由等功能，
    可以帮助开发人员构建高可用、高性能、易于管理的分布式系统。Dubbo还提供了丰富的扩展点，可以方便地扩展和定制各种功能，比如自定义协议、
    序列化器、负载均衡策略等。
    
    Dubbo的特点包括：
    	高性能：Dubbo采用了多种优化方案，比如基于Netty实现的NIO通信模型、线程池、请求响应协议等，以保证高性能的RPC调用。
    	透明化：Dubbo提供了透明化的RPC调用方式，让开发人员可以像调用本地服务一样调用远程服务，无需关心底层的通信细节和协议。
    	高可用：Dubbo提供了多种容错机制，比如Failover、Failfast、Failsafe等，可以在服务出现故障时自动切换到备用服务，保证系统
    		   的高可用性。
    	高扩展：Dubbo提供了丰富的扩展点，可以灵活地扩展和定制各种功能，比如协议、序列化器、负载均衡策略等。
    
    	Dubbo适用于构建大规模、高可用、高性能的分布式系统，尤其是微服务架构中的服务治理和服务调用。它可以用于各种场景，比如电商、金融、
    物流等领域的分布式系统。

## 10.说下你对Mysql事务的理解？

```
1.什么是事务？
	事务就是指一组不可分割的操作视为一个不可分割的单元格，这些操作要么全部成功，要么全部失败
2.事务的四大特征
	原子性：事务要么同时成功，要么同时失败
	一致性：数据前后保持一致
	隔离性：一个事务的执行不能被其他事务干扰 
	持久性：数据一旦修改，就会一直保存
3.事务的隔离级别
	Oracle事务隔离级别: 	不可重复读
	MySQL事务隔离级别：可重复读


脏读(读未提交)：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据（读了回滚前数据）
不可重复读（读已提交）：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致。(A多次读取，B更新修改，数据不一致)（Oracle）
幻读（可重复读）：事务a,买了100元东西。查看记录花了100，事务b同时也用事务a的号买了200元东西，结果事务a一看他花了300元，他就会怀疑自己（打印资料的事）（B新增了一条记录，A感知到发现数据不一致）
串行化：

Ps:不可重复读侧重于修改，幻读侧重于新增或删除
	
```

## 11.如何进行数据库的优化？

    	数据库的优化分为四大类，sql的优化、数据库结构的优化、系统配置、和硬件
    	数据库结构的优化一般是表结构的优化 三范式 ，三范式的要求其实就是减少冗余字段 一般企业不会
    严格遵循三范式 这样就经常需要表连接 所以企业一般设计表的时候 一个功能最好设计在一个表上尽量减少
    表连接带来的性能影响、系统配置 主要是配置一些策略 硬件的话 那就是一劳永逸的事情了 但是这样的成本
    也是最高的综合效率和成本来说那最重要的就是sql优化了
    
    一：**Sql优化： 不用select * 这样的
    （1）语句执行顺序：where having
    	1.能使用where就不要having，where他是带着条件查询的他会把结果集直接过滤，having则是先分组，再过滤结果集。
    Where不能使用聚合函数，groupby分组的时候,只能显示分组字段、聚合函数字段
    
    （2）多表连接优化：加字段  延迟加载
    	1.多表操作转换成当表操作 在当表上加个字段 user表加一个accountids 保存用户对应的所有账户id(id字符串需要切割处理)
    	2.mybatis的延迟加载  但是会产生n+1的问题
    	3.小表驱动大表，可以省去mysql优化器判断的时间，表连接的字段加索引 
    
    （3）关键字优化  exist替换in
    	1.in会进行全表扫描  exist,in后面的数据占比比较大的时候，不会走索引
    	2.union代替or,避免全表扫描
    
    （4）索引优化：避免not like ，他们依然会全表扫描
    	1.不要用select*,要尽量指定字段,指定字段的情况下有时候是可以走覆盖索引的，这样就减少了回表的次数，效率也会更高
    	【聚簇索引也就是我们的主键索引，叶子节点保留了具体的行数据，非聚簇索引又叫辅助索引，如果说辅助索引上面的数据符
    	合查询的数据，那么就不用回表到主键索引上面去拿全部数据了，这也叫做覆盖索引】
    
    索引：底层b+tree  在表的字段上面创建  查询效率很高（它是一种数据结构会占用存储空间的）
    
    单列索引、复合索引
    什么时候创建索引：
    1.数据量比较大的表适合使用
    2.经常查询的（查询条件）的字段使用索引
    3.经常增删改的数据不适合索引（树会重建影响效率）
    		数据库的资源是很宝贵的，用户每次请求都会数据库的数据都会变化，数据库压力很大，数据库容易出现瓶颈，
    	所以操作数据库的时候，希望你能够尽快的建立连接释
    	放连接，少数据库的压力
    
    （5）我们还可以用Explain打印sql的执行计划，查看是否走没走索引，走了哪个索引，有没有用临时表来进一步根据情况
    	来加索引或者修改复合索引
    当数据量足够大，索引、缓存已经无法明显提高效率的时候，就可以考虑分库分表  读写分离

## 12.说下你对Redis的理解？

    1.什么是redis
    	Redis是C语言写的基于内存存储的一种数据库，查询速度非常的快，常用于缓存、分布式锁、消息队列
    
    2.redis的特点和数据类型
    3.各个数据类型应用的场景
    	String：key，value 二进制的形式存储数据，常用作缓存微博数、粉丝数
    	List：  (key,list) 微博关注列表、粉丝列表、消息列表、朋友圈点赞
    	Hash： 数组+链表( key , key value) 用来存储用户信息，商品信息,购物车等
    	Set：   key，value 共同关注、共同粉丝  集合的差 并 交
    	Zset：  排行信息,可以根据字段进行排序
    
    4.和memcached区别
    共同点：
    	- 都是基于内存的缓存
    	- 都有过期策略
    	- 性能都非常高
    区别：
    	- Redis支持更多丰富的数据类型 Sting、list、hash、set、zset，memcache只支持string类型
    	- Redis支持数据持久化，将数据保存在磁盘中，重启时再次加载使用，memcache把数据全部存储在内当中，不支持持久化
    
    5.8种淘汰策略
    	(1)作用：Redis的淘汰策略主要是为了解决内存不足的问题，它可以在内存达到一定阈值时淘汰一些不常用的键值对，以释放内存空间。
    	(2)Redis支持的淘汰策略主要有以下8种：「1.默认  2.针对全部key 3.针对过期时间key的」
    		- （默认策略）noeviction（驱逐）：当内存空间不足以容纳新写入数据时，新写入操作会报错。
    
    		- allkeys-lru：当内存空间不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。
    		【全部key最近最少使用】
    
    		- allkeys-lfu：当内存空间不足以容纳新写入数据时，在键空间中，移除最不经常使用的key。
    		【全部key使用频率最少的】
    
    		- allkeys-random：当内存空间不足以容纳新写入数据时，在键空间中，随机移除某个key。
    		【全部key随机移除key】
    
    		- volatile-lru：当内存空间不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。
    		【设有过期时间的key,最近最少使用】
    
    		- volatile-lfu：当内存空间不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最不经常使用的key。
    		【有过期时间的key,使用频率最少的】
    
    		- volatile-random：当内存空间不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。
    		【设有过期时间的key,随机删除】
    
    		- volatile-ttl：当内存空间不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先被移除。
    		【过期时间最短的】
    
    6.过期策略
    	（1）惰性删除:
    			指当一个键过期后，这个键只有在被访问时才会被删除。当Redis尝试获取一个已经过期的键时，会发现这个键过期了，
    		然后就会删除它并返回空结果给客户端
    			优点：可以节省删除过期键的 CPU 时间
    			缺点：键很久没被访问，那么它将一直占用内存，直到被访问时才能被删除
    	（2）定时删除:
    			Redis会每隔一段时间（默认是100ms）去扫描一定数量的过期键，并将这些过期键删除
    			优点：可以保证过期键及时被删除
    			缺点：会占用一些CPU时间和IO时间，因此需要合理设置删除频率
    
    7.redis的持久化策略
    	RDB:又叫做Redis数据快照是Redis默认的持久化机制，停机的时候默认会执行
    save方法触发RDB机制，他也可以配置每隔一段时间执行bgsave方法进行一次RDB持久化数据，文件体积较小、宕机恢复速度很快，
    但是两次数据备份期间可能会丢失数据，如果允许数分钟的数据丢失，追求更快的启动速度我们会选择它
    	AOF:他是记录每一次执行命令的方式进行存储的，基于他的刷盘策略我们可以选择执行一条命令进行一次持久化、或者每秒进
    行一次持久化AOF文件会比较大，我们可以根据bgrewriteaof命令对AOF文件进行重写或者配置达到一定的内存自动进行重写（可
    以理解为压缩文件，将重复的命令剔除）减少AOF文件的大小，它的容灾恢复速度比不上RDB,但是他的数据保存的会更加完整、更加
    安全，如果追求数据少丢失（安全）的情况我们会选择它
    
    8.缓存穿透、雪崩、击穿
    （1）缓存穿透：查询一个一定不存在的数据，数据不存在自然也不会缓存，导致请求直接到了数据库上，根本没有经过缓存这一层
    （数据库压力很大，恶意攻击就会数据库奔溃）
    缓存穿透，将所有可能存在的数据缓存放到布隆过滤器中，当黑客访问不存在的缓存时迅速返回避免缓存及DB挂掉。
    解决：布隆过滤器布隆过滤器(bitmap\内存占用小)或者（数据库为空也加入缓存，key可以是提交过来的主键，值为null，
    设置较短的时间60秒，但是如果有人进行不同的数据攻击会造成redis内存占用比较高）
    
    （2）缓存雪崩：缓存在同一时间失效，后面的请求都直接落到数据库上，造成数据库短时间内承受大量请求或者有一些被大量访问的
    数据在某一时刻大面积失效，导致对应的请求直接落到数据库上（某一时间段，缓存大量过期并发一大，请求全部到了数据库）
    解决：采用集群(保证redis的高可用)或者限流（热门数据设置缓存较长时间，特别热门的数据可以设置永不过期，或者设置时间
    不要一样这样可以做限流的作用，利用算法错开设置有效时间）
    
    （3）缓存击穿：某一时间点，热门数据的key失效，高并发就会使数据库崩溃
    解决：热门数据永不过期或者加锁【大量并发【缓存数据为空】只让一个去查，其他人等待，查到以后释放锁，其他人获取到锁，
    先查缓存，就会有数据，不用去db】
    
    9.集群主从模式 、哨兵机制、cluster
    	数据同步原理（主从复制的原理）：
    	（1）全量同步：1.主节点判断是否第一次同步 2.异步执行bgsave将rdb文件发给slave  3.rdb期间的命令通过缓冲区
    		repl_baklog慢慢发送给slave【master将完整的内存生成RDB文件、发送RDB给slave。后续的命令则记录在
    		repl_baklog逐个发送给slave】
    	（2）增量同步：slave提交自己的offset到master,master获取repl_baklog中offset后面的命令给slave
    
    	什么时候进行全量同步？
    	（1）slave节点第一次连接master的时候
    	（2）slave节点断开太久，repl_baklog中的offset已经被覆盖时
    
    	什么时候进行增量同步？
    	（1）slave节点断开又恢复，并且在repl_baklog中能找到offset时
    
    主从模式：
    	（1）背景：
    	在单节点的并发能力是有上线的，并且单节点如果挂了会导致整个服务都不可用，再次基础上引入主从模式
    提高系统的并发量，实现读写分离（主节点负责写，从节点负责读），从节点会定时向主节点发送偏移量利用全量同步
    或者增量同步主节点的数据
    	（2）存在的问题：
    	如果主节点宕机，从节点将无法与主节点通信，这将导致从节点无法获得来自主节点的更新数据，为了避免数据丢失和服务
    中断，我们一般使用哨兵模式解决此类问题
    
    哨兵模式：
    	Redis哨兵机制是一种用于处理Redis主从复制模式中主节点故障的自动故障转移解决方案
    	（1）三大特征：
    		监控(Monitoring)：不断检查主从服务器（哨兵监测主从节点，心跳机制看看他们是否存活）
    		提醒(Notification)：通过API向管理员或其他应用程序发通知或者警报，以便管理员能采取相应的措施
    		自动故障迁移(Automatic failover)：当主服务器宕机，哨兵会开始一次自动故障迁移(迁移时不进行服务，迁移需要时间)
    	   （主节点下线，某个从节点升级为主节点，继续处理，其余从节点开始从新的主节点复制数据）  投票机制
    	（2）哨兵机制还支持配置多个哨兵进程，以提高系统的可靠性。多个哨兵进程可以相互通信，并通过共同决策来选择新的主节点，
    		从而避免单点故障。
    
    Redis-Cluster集群（分片集群）
    	（1）背景：
    			主从和哨兵可以解决高可用、并发读的问题，但是无法解决1.海量数据存储的问题 2.高并发写的问题
    	（2）为什么选择分片集群
    			分片集群采用多主多从的结构，不需要哨兵也可以实现哨兵的类似效果，每个master直接有心跳机制
    			保证彼此可用，同时每个master也可以配置主从，数据分配到多个master解决了海量存储的问题，多个
    			master解决了高并发写的问题
    	（3）哈希槽原理（数据是与槽进行绑定的，而不是节点）
    			通过CRC算法计算key的hash值，然后与16384取余得到slot槽值，然后然后根据槽的位置判断属于哪个
    		redis节点上，再重定向到对应槽的节点上进行操作
    		
    		常见问题：
    		（1）如何将同一类数据固定的保存在同一个Redis实例?
    			控制有效部分：这一类数据使用相同的有效部分，例如key都以{typeld}为前缀
    			【设置key的前缀用{}括起来，作为有效部分】
    
    		（2）集群伸缩如何实现
    			增加节点：使用 CLUSTER MEET 命令可以将新的节点添加到集群中，该命令的语法如下：
    					CLUSTER MEET <ip> <port>
    				
    					其中，`<ip>` 和 `<port>` 分别是要添加的节点的 IP 地址和端口号。当新节点加入集群后，
    					集群会自动将其分配到合适的槽位中，并进行数据迁移。
    
    			删除节点：使用 CLUSTER FORGET 命令可以将指定节点从集群中删除，该命令的语法如下：
    					CLUSTER FORGET <node_id>
    			其中，`<node_id>` 是要删除的节点的 ID。当节点被删除后，集群会重新分配其所负责的槽位，
    			并进行数据迁移。
    
    		（3）故障迁移如何实现
    			使用 Redis Cluster 自身的数据迁移机制，将新节点分配到的槽位中的数据迁移到新节点上。
    			当新节点接管了这些槽位的数据后，数据迁移就完成了。在数据迁移期间，Redis Cluster 会
    			自动保证数据的一致性和可用性，不会影响集群的正常运行
    
    10.redis和数据库为什么会出数据不一致的情况，如何解决不一致的情况
    	双写模式：数据库数据修改时，连同缓存一起修改
    	失效模式：数据库数据修改时，删除缓存数据，下次访问数据的时候依旧会将新的数据放入缓存 
    	问题：双写模式和失效模式都会有脏数据的风险
    
    	解决：
    		1.缓存的所有数据都有过期时间，数据过期下一次查询触发主动更新
        	2.读写数据的时候加上分布式的读写锁【读写锁的特点：读写互斥，读读共享】，可以保证读的时
    	候永远都是最新数据【如果先进行读操作会先读完再写，如果写操作先，那就会写完再读】ReentrantReadWriteLock  
     
    11.如何基于Redis实现分布式锁
    	背景：
    		分布式锁主要就是为了保证一个方法或属性在高并发情况下的同一时间只能被同一个线程访问资源，
    	在传统单体应用下一般加synchronized锁就可以解决，现在有了分布式的系统项目部署在多个机器上面这种
    	锁就会失效，所以需要分布式锁来解决现在的问题。
    		（1）基于数据库实现
    			乐观锁，新增版本号字段，每次更新都携带版本号更新（和CAS很像），不成功会进行自旋重新携带更新后的
    			版本号继续更新
    		（2）基于Zookeeper实现
    			Zookeeper实现的锁性能较好，但是不如redis,而且复杂度比较高
    		（3）基于Redis实现
    			我最常用的也是也是Redis的实现（Redisson解决）
    		解决：
    			Redisson底层是lua脚本语言进行加锁的，lua脚本在redis当中会被当做一条原子操作命令。
    		加锁成功的时候后台会创建一个守护线程（也就是我们俗称的看门狗），默认30秒的三分之一（10秒）执行一次，
    		判断锁是否存在，存在则给锁重新设为30秒给锁续命。【可以保证业务没执行完锁不会自动释放或者服务宕机所没有释放的问题】
    		加锁失败的时候会进行CAS自旋，进行锁的抢占。

## 13.JVM基本知识和类装载机制？

    1.什么是JVM？
    	JVM又称java虚拟机，他是一个抽象的计算机，他能将java代码编译生成字节码文件。
    
    2.它的核心特点：
    	跨平台性：一次编译到处运行，生成的字节码可以在各个支持JVM的平台上运行
    
    3.类加载机制
    	类加载的意义：
    		类加载机制主要做的事情就是：将.class文件，加载进内存，最终形成class对象，初始化里面的静态成员
    		【也就是加载、链接、初始化】
    
    	三个类加载器：
    		加载.class文件的时候会去初始化JVM三个类加载器【职责明确，各自加载各自的】
    			1.启动类加载器   （底层C++写的）   只能加载java核心类库    jre\lib\rt.jar                                                          
    			2.扩展类加载器                   只能加载扩展包类库          jre\lib\ext\*.jar                                          
    			3.系统类加载器    			   只能加载自己写的类（classpath下的类 src下）  
    		双亲委派机制：
    				当一个类收到了类加载请求，他首先不会尝试自己去加载这个类，而是把这个请求委派给父类去完成，
    			每一个层次类加载器都是如此，只有当父类加载器反馈自己无法完成这个请求的时候(在它的加载路
    			径下没有找到所需加载的Class) ，子类加载器才会尝试自己去加载。 
    		
    		双亲委派机制优点：
    			避免类重复加载
    				例子：父类加载器加载过了，子类加载器就不会加载这个类了
    			保护程序安全，防止核心API被随意篡改【沙箱安全机制】
    				例子：如果定义一个java.lang.String类，他本应该是系统类加载器去加载，但是双亲委派到启动类加载器
    				的时候发现这个类是java.lang，他会加载JDK自带的String类而不会加载这个自定义的类【防止篡改】
    		
    		自定义类加载器
    			所有用户自定义类加载器都应该继承ClassLoader类。在自定义ClassLoader的子类是,我们通常有两种做法:
    			【严格上来说我们的扩展类加载器和系统类加载器都是自定义加载器，他们底层都间接继承了ClassLoader类】
    				重写loadClass方法(是实现双亲委派逻辑的地方,修改他会破坏双亲委派机制,不推荐)
    				重写findClass方法 (推荐)
    
    	类加载的生命周期:
    		(1)加载，通过类的全限定类名获取它的二进制字节流并加载类的二进制数据，在Java堆中也创建一个
    				java.lang.Class类的对象【加载	二进制流创建class对象】
    		(2)链接，连接又包含三块内容：验证、准备、解析。 
    			1）验证，文件格式、元数据、字节码、符号引用验证【确保class文件的字节流信息符合jvm口味，校验信息】
    			2）准备，为类的静态变量分配内存，并将其初始化为默认值【静态初始化】
    			3）解析，把类中的符号引用转换为直接引用 
    		(3)初始化，为类的静态变量赋予正确的初始值【动态初始化】
    			静态代码块只会被加载一次，底层是通过clinit()函数实现的，静态代码块会被clinit()函数包裹
    			clinit()函数是线程安全的，这个线程安全是JVM保证的
    		(4)使用：new出对象程序中使用【实例变量实在创建对象的时候分配内存的】
    		(5)卸载：执行垃圾回收
    		
    4.GC机制的理解
    	(1)意义：程序运行就要申请内存资源，无效的对象如果不及时处理的话就会一直占用内存资源，最终导致内存溢出，
    		管理内存资源是很重要的
    
    	(2)JVM内存管理分为5大区域
    		（1）方法区：用于存储类信息、常量、静态变量等数据
    		（2）堆区：是 JVM 管理的最大的一块内存区域，用于存储对象实例和数组
    		（3）虚拟机栈：每个线程都有自己的栈，用于存储方法调用时的局部变量、操作数栈、返回值等信息
    		（4）本地方法栈：存储本地方法的信息
    		（5）程序计数器：是一块较小的内存区域，用于保存当前线程执行的字节码指令地址
    	垃圾回收主要针对内存比较大的堆内存进行
    		堆分为新生代和老年代，占比1:3；新生代又分为，一个eden 8/10,两个幸存者1/10
    		- 新生代（1/3） 所有新创的对象放入eden空间，满了进行Minor GC，将存活的对象进行复制算法放入幸存者空间
    		 （Survivor 减少被送到老年代的对象，进而减少FullGC 的发生。），循环+1（默认15次）
     		- 老年代（2/3） 15次之后还存活的对象会放入老年代、或者一些大对象直接进入老年代比如比较大的字符串、数组，
    		  老年代满了会进行FullCG，利用标记整理算法，先标记所有被引用的对象，再遍历整个堆，把存活下来的对象压
    		  缩到堆中的其中一块，按顺序排放【标记整理算法】，进行FullGC的时候会出现STW现象(除了GC所需的线程以外，
    		  所有线程都处于等待状态直到GC任务完成)
    
    	（3）如何判断内存对象需要回收(对象是否可以被回收的两种经典算法: 引用计数法 和 可达性分析算法)
    		- 「引用计数算法」：堆中的每个对象实例都有一个引用计数属性，被引用引用对象 +1 ，引用断开-1，为0标记为无用对象，
    		  清除无用对象（他是JVM早期的垃圾回收策略，他不能解决对象之间相互引用的问题   例如：两个对象A、B他们相互引
    			用，用这种方法，AB两个对象都无法被垃圾回收）
    		- 「可达性分析」：GCroot往下搜索，走过的路径叫引用链，A-B-C  比如C有引用链，说明他是可用对象，就不清理；如果没
    		  有引用，那么就认为从 GC Roots 到这个对象不可达的，也就是没有引用链那么这个对象就会被回收
    
    	（4）如何回收、哪些回收算法(三种经典垃圾回收算法(标记清除算法、复制算法、标记整理算法)及分代收集算法 和 七种垃圾收集器
    		1.标记-清除算法：该算法分为“标记”和“清除”阶段：标记被引用的对象，清除未标记的对象。
    			问题：
    			效率问题	
    			空间问题（标记清除后会产生大量不连续的碎片）
    
    		2.复制算法：复制算法将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的
    		  对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉【适用于对象存活率低的场景，比如新生代】
    		【两块，使用一块，存活的对象会放到下一块，清除之前使用过的内存】
    
    		3.标记-整理算法：：结合了标记清除与复制两个算法的特点，先标记所有被引用的对象，再遍历整个堆，把存活下来的对象压缩到堆
    		  中的其中一块，按顺序排放【压缩】，避免了标记清除的碎片问题与复制算法的空间问题。【用于对象存活率高的场景（老年代）】
    		【标记引用对象，压缩存活对象，按顺序存放】
    
    		4.分代收集算法：根据对象存活周期的不同将内存分为几块。一般将 java 堆分为新生代和老年代，这样我门就可以根据不同的年代
    		  选择不同的垃圾回收算法。
    
    	（5）七种垃圾回收器如何选择
    		1.CMS(Concurrent Mark Sweep)收集器（标记-清除算法）： 老年代并行收集器，以获取最短回收停顿时间为目标的收集器，
    		具有高并发、低停顿的特点，追求最短GC回收停顿时间
    		2.G1收集器是JDK1.7提供的一个新收集器，G1收集器基于“标记-整理”算法实现，也就是说不会产生内存碎片。
    		  G1回收的范围是整个Java堆，两个特点：就是把java堆内存拆分为多个大小相等的Region,可以让我们设置一个垃圾回收的预期停顿时间

## 14.RabbitMq常见问题解决方式？

```
1.RabbitMq解决消息丢失的问题
	首先，消息丢失要从三个方面考虑，生产者丢消息、Mq丢消息、消费者丢消息
		(1)针对生产者：
			a.开启RabbitMq事务机制，不推荐，同步并且效率低(性能下降200倍)
			b.开启RabbitMq消息确认机制(两个回调方法)
				- 利用rabbitTemplate.confirmCallback【消息被broker接收到ack=true】
				- 利用rabbitTemplate.setReturnCallback【消息到达队列确认回调】
			可以通过判断replyCode的值来确定消息是否发送失败了。当replyCode为312时，表示消息未被路由到指定的Queue，
			即消息发送失败；当replyCode为313时，表示消息未被路由到指定的Exchange，同样也表示消息发送失败。
		(2)针对Mq：
			Mq开启持久化（保证断电、宕机各种情况下消息不丢失）：
				- 队列持久化：创建队列时设置durable = true来实现
				- 消息持久化：发送消息时设置MessageProperties的deliveryMode属性来实现，
					deliveryMode属性的值为2时表示消息需要持久化，值为1时表示消息不需要持久化
		(3)针对消费者： 
			消费者开启手动ACK：
				- channel.basicAck(message.getMessageProperties().getDeliveryTag(), false) 
					参数1：deliveryTag消息唯一标识  参数2：是否批量确认
				— channel.basicReject()方法拒绝消息并重新入队，以便重复处理
					参数1：deliveryTag消息唯一标识  参数2：是否重新入队

2.RabbitMq解决消息重复消费的问题
	(1)消息去重（一般用这个，redis横向扩展也方便，支持高并发访问）
		对消费端已经消费过的数据进行标记，消费完再redis缓存消息的唯一标识和过期时间1天，下次再消费的时候判断缓存中是否存在就行了
	(2)消费端做幂等
		a.数据库唯一约束：建一张表保存消息，并对消息的唯一标识创建唯一索引，消费成功的时候进行入库，下一次相同消息进
	来先根据唯一标识去数据库查询，存在则直接ack

3.RabbitMq如何解决消息堆积的问题
	1.增加消费者数量，对某个队列进行消费
		

```

## 15.RocketMq常见问题解决方式？

    （1）消息发送确认机制：通过配置 producer.sendMsgTimeout 参数，可以设置消息发送超时时间，如果消息在规定时间内
    	没有得到确认，将触发重试机制。此外，还可以通过配置 producer.retryTimesWhenSendFailed 参数，设置消息发送失败
    	后的重试次数，确保消息能够被成功发送。
    	-【消息发送机制：规定时间内没收到，可以重试，还可以设置重试次数保证消息能被接收】
    
    （2）消息消费确认机制：RocketMQ 提供了两种消费确认方式：自动确认和手动确认。自动确认方式是默认的确认方式，消费者
    	收到消息后立即确认消费。手动确认方式可以通过配置 consumer.consumeThreadMax 参数，设置消费线程池的最大值，
    	以及 consumer.maxReconsumeTimes 参数，设置消息消费失败后的最大重试次数。如果消费者在处理消息时发生异常或
    	超时等情况，可以手动调用 ConsumeConcurrentlyStatus.RECONSUME_LATER 方法进行消息重试。
    	- 【消息确认机制：默认是自动确认，改成手动确认，防止消息丢失】
    
    （3）消息重复消费机制：RocketMQ 通过消息消费的唯一标识（Message ID 或者 Consume Queue Offset）来保证消息不
    	会被重复消费。消费者在消费消息时需要保存消费的位置信息，并在消息消费完成后更新消费位置信息。此外，RocketMQ 还
    	提供了顺序消费的特性，可以保证同一个消费者在同一个队列中消费的消息顺序不会错乱。
    	-【可以通过消息唯一Id来做幂等操作，或者放redis判断是否存在】
    
    （4）消息持久化机制：RocketMQ 的消息存储采用了类似于 Kafka 的设计思路，将消息存储在磁盘中，并支持消息的持久化和
    	重放。通过配置 store.syncFlush=true 参数，可以设置消息同步刷盘，确保消息能够被持久化到磁盘中。
    	- 【消息持久化机制：可以设置同步刷盘，将消息保存到磁盘中】
    
    （5）高可用性机制：RocketMQ 支持多节点部署，通过主从同步等机制实现高可用性。当主节点宕机时，从节点会自动切换为
    	主节点，确保消息服务的可用性。
    	-【高可用机制：可以搭建主从集群做到高可用】

## 16.Docker的常见使用方式有哪些？

    1.为什么要使用docker?
    	传统的虚拟机在部署项目的时候，运维人员需要将项目按照开发人员的之前的环境进行部署，可能会因为版本等环境问题导致项目部署失败，
    给运维人员增添了很多不必要的负担，并且虚拟机比较重、启动慢、占用内存也比较大
    
    2.docker是什么？解决了什么问题？
    	Docker 是一种容器化技术，可以将应用程序及其环境打包成一个可移植的容器，从而可以在不同的环境中运行，达到一次镜像导出运行的作用
    ，运维人员也不用担心部署环境不同而增添不必要的负担，Docker容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统虚拟
    机则是在硬件层面实现虚拟化。与传统的虚拟机相比，Docker 优势体现为启动速度快、占用体积小。
    
    3.常见的命令操作
    (1)镜像命令
    	- 查询本地所有镜像   docker images          [参数 -a 列出本地所有镜像（含历史镜像层） 参数 -q 只显示镜像Id]
    	- 查询某个镜像      docker search 镜像名称         
    	- 拉取某个镜像      docker pull 镜像名称[:TAG]     
    	- 查看镜像、容器、数据卷所占的空间      docker system df              
    	- 删除某个镜像      docker rmi 镜像Id 				
    		-删除单个docker rmi -f 镜像id  删除多个docker rmi -f 镜像名1:TAG 镜像名2:TAG 全删除docker rmi -f $(docker images -qa)
    
    (2)容器命令[有镜像才能创建容器]
    	- *新建+启动容器    	 docker run [OPTIONS] 
    							--name 为容器命名  -it 启动交互式容器 -p 宿主机端口：容器端口   /bin/bash 运行容器
    
    	- *列出当前正在运行的容器 docker ps [OPTIONS] 	-a :列出当前所有正在运行的容器+历史上运行过的		
    												-l :显示最近创建的容器。
    												-n：显示最近n个创建的容器。
    												-q :静默模式，只显示容器编号。
    	- 退出容器             run进去  1.exit退出，容器停止  2.ctrl+p+q退出，容器不停止	
    	- 启动已经停止运行的容器 docker start 容器Id或容器名
    	- 重启容器             docker restart 容器Id或容器名
    	- 停止容器             docker stop 容器Id或容器名
    	- 强制停止容器          docker kill 容器Id或容器名
    	- 删除停止的容器        docker rm 容器Id
    	*重要*
    	- *启动守护式容器
    		- 将你要运行的程序以前台进程的形式运行，常见就是命令行模式，表示我还有交互操作，别中断
    		- redis前后台启动    1.前台交互式启动：docker run -it redis:6.0.8  2.后台守护式启动 docker run -d redis:6.0.8
    	- 查看容器日志          docker logs 容器Id 
    	- 查看容器内部运行进程   docker top 容器Id 
    	- *查询容器内部细节（配置） docker inspect 容器Id
    	- *进入正在运行的容器并以命令行交互 	1.推荐：docker exec -it 容器Id /bin/bash
    									2.重新进入docker attach 容器Id
    								区别：attach直接进入容器启动命令的终端，不会启动新的进程，用exit退出，会导致容器停止
    									 exec是在容器中打开新的终端，并且可以启动新的进程，用exit退出，不会导致容器停止
    								一般用-d后台启动程序，再用exec进入对应容器实例
    	- *从容器内拷贝文件到主机上   容器-->主机   docker cp 容器Id:容器内路径 目的主机路径
    	- 导入和导出容器
    		export「将容器内容打包成TAR包」: docker export 容器Id > 文件名.tar
    		import「将TAR转换成镜像」: cat 文件名.tar | docker import-镜像用户/镜像名:镜像版本
    （3）镜像命令：
    	- 将容器副本做成新的镜像提交
    		docker commit  -m="提交的信息描述" -a="作者" 容器Id 要创建的目标镜像名:[TAG]


    4.docker网络
    5.dockerfile如何使用
    6.docker compose如何使用
    7.什么是docker虚悬镜像?
    	仓库名和标签（版本）都是<none>的镜像

## 17.说下你对K8s的了解？

```
```

## 19.原子操作AtomicLong和LongAdder使用过吗？

    1.AtomicLong
        原理
            CAS+自旋
            incrementAndGet
        场景
            低并发下的全局计算
            AtomicLong能保证并发情况下计数的准确性，其内部通过CAS来解决并发安全性的问题。
        缺陷
            高并发下性能急剧下降
            why?
                AtomicLong的自旋会成为瓶颈
                    N个线程CAS操作修改线程的值，每次只有一个成功过，其它N,-1失败，失败的不停的自旋直到成功，这样大量失败自旋的情况，一下子cpu就打高了。
    
    2.LongAdder
        原理
            CAS+Base+Cell数组分散
            空间换时间分散热点数据
        场景
            高并发下的全局计算
        缺陷
            sum求和后还有计算线程修改结果的话，最后结果不够准确
    
    	使用：
    	public class PointSystem {
        private LongAdder totalPoints = new LongAdder();
    
        public void addPoints(int pointsToAdd) {
            // 累加用户积分
            totalPoints.add(pointsToAdd);
        }
    
        public long getTotalPoints() {
            // 获取总积分
            return totalPoints.longValue();
        }
    }

## 20.说下你对ThreadLocal的理解？

```
1.什么是ThreadLocal，能干什么？
   ThreadLocal又叫做线程变量，它使每个线程都有一份自己独立的副本，实现线程隔离，不存在多线程共享的问题
2.适用场景
   ThreadLocal 适用于每个线程需要自己独立的实例且该实例需要在多个方法中被使用
3.线程池操作【阿里规范】
    【强制】必须回收自定义的ThreadLocal变量，尤其在线程池场景下，线程经常会被复用，如果不清理自定义的
ThreadLocal变量，可能会影响后续业务逻辑和造成内存泄露等问题。尽量在代理中使用try-finally块进行回收
「线程池复用的话，假如ThreadLocal默认值为0，第一次加一你不清除，第二次同样的线程池拿到这个值就不是初始值了，
	而是被加一的值了」

4.threadLocal底层原理
	(1)ThreadLocal本身并不存储数据,底层是使用一个静态内部类的ThreadLocalMap进行存储的。
	(2)当调用ThreadLocal的set(Tvalue)方法时，ThreadLocal将自身的引用也就是this作为Key，然后，把用户
		传入的值作为Value存储到线程的ThreadLocalMap中，这就相当于每个线程的读写操作都是基于线程自身的一个
		私有副本，线程之间的数据是相互隔离的，互不影响。
		「ThreadLocalMap  key:当前ThreadLocal引用 value:副本变量」
	(3)ThreadLocalMap中使用的key为ThreadLocal的弱引用，弱引用的特点是，如果这个对象只存在弱引用，那么
		在下一次垃圾回收的时候必然会被清理掉。 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候会被
		清理掉的，这样一来 ThreadLocalMap中使用这个 ThreadLocal 的 key 也会被清理掉。但是，value 是
		强引用，不会被清理，这样一来就会出现 key 为 null 的 value。
	(4)存在这种脏Entry,key=null,value还有数据,调用ThreadLocal.remove（）方法，清理脏entry解决内存泄漏的问题


常见的大厂面试题
        ThreadLocal中ThreadLocalMap的数据结构和关系?
        ThreadLocal的key是弱引用，这是为什么?
			- 背景：在使用ThreadLocal时，如果我们在某个线程中使用了一个ThreadLocal变量并且没有手动
				   地将其从ThreadLocalMap中移除，那么这个变量将会一直存在于ThreadLocalMap中，而这
				   个ThreadLocalMap对象又是与当前线程相关的。这就意味着，即使这个线程已经结束了，但是
				   这个ThreadLocalMap对象还是会一直存在于内存中，而其中的变量也会一直存在，并可能导致内存泄漏问题。
			- 解决：1.ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候会被清理掉的，这样一来 
					ThreadLocalMap中使用这个 ThreadLocal 的 key 也会被清理掉。但是，value 是
					强引用，不会被清理，这样一来就会出现 key 为 null 的 value。
				   2.调用remove（）方法
					
        ThreadLocal内存泄露问题你知道吗?
			- 脏entry
        ThreadLocal中最后为什么要加remove方法? 
			- 解决内存泄露问题
		                                                                                                                                                                                  
```

## 21.**private default protect public的作用范围**

```
private：Java语言中对访问权限限制的最窄的修饰符，一般称之为"私有的"。
		被其修饰的类、属性以及方法只能被该类的对象访问，其子类不能访问，更不能允许跨包访问。

default：即不加任何访问修饰符，通常称为"默认访问模式"。
		该模式下，只允许在同一个包中进行访问。

protect：一般称之为“受保护的”。被其修饰的类、属性以及方法
		只能被类本身的方法及子类访问，即使子类在不同的包中也可以访问。

public：Java语言中访问限制最宽的修饰符，一般称之为"公共的"。
		被其修饰的类、属性以及方法不仅可以跨类访问，而且允许跨包（package）访问。

```

## 22.导出的报表的时候遇到百万的数据怎么办

    一般思路：
    1.技术选型poi、easyPoi、easyExcel等等
    2.多线程加载数据
    3.放入redis在进行数据处理
    4.分批次导出

# 项目问题

## 0.项目描述和介绍

```
	面试官(崔总)你好，我叫李钰斌，之前有近三年的开发经验，我熟悉ssm框架、SpringCloud微服务框架、
Juc并发编程、redis、RabbitMq,知道一些数据库的优化、常用的设计模式、JVM类加载机制，会用一些常用的linux命令，平时喜欢看一些网上的博文，面对新的技术也喜欢去探索玩一下，也喜欢团体活动

	我最近的一个项目是一个对标华为云的saas项目，随着深信服安全产品XaaS化的推进，为渠道/用户的在线购买
云主机、防火墙等商品提供在线计费的能力，从而进一步扩宽市场影响力、增加收益，主要的功能有对渠道、客户提供
订单管理、账单管理、账户管理等信息，为运营、销售、财务提供订单中心、商品中心、营销中心、账单中心、消息中心等
服务，主要的技术栈采用的是SpringCloudAlibaba那一套，以及Redis、RabbitMq、DDD领域驱动设计的思想
	我负责的主要是创建订单的重构优化、秒杀、购物车，订单支付、订单完成等操作
 

```

## 1.使用RabbitMq优化创建订单的接口进行异步解耦，对消息丢失、重复消费等问题进行解决

```
1.你的项目如何设计的？
创建订单采用的是路由模式，交换机exchange = direct,routingKey = 'xaas.order.createOrder'，每个路由键绑定了一个队列，目前来说的话
没有性能的问题，如果消息堆积的话，我们可以增加队列绑定路由键，提高消费水平

1.RabbitMq解决消息丢失的问题
	首先，消息丢失要从三个方面考虑，生产者丢消息、Mq丢消息、消费者丢消息
		(1)针对生产者：
			a.开启RabbitMq事务机制，不推荐，同步并且效率低(性能下降200倍)
			b.开启RabbitMq消息确认机制(两个回调方法)
				- 利用rabbitTemplate.confirmCallback【消息被broker接收到ack=true】
				- 利用rabbitTemplate.setReturnCallback【消息到达队列确认回调】
			可以通过判断replyCode的值来确定消息是否发送失败了。当replyCode为312时，表示消息未被路由到指定的Queue，
			即消息发送失败；当replyCode为313时，表示消息未被路由到指定的Exchange，同样也表示消息发送失败。
		(2)针对Mq：
			Mq开启持久化（保证断电、宕机各种情况下消息不丢失）：
				- 队列持久化：创建队列时设置durable = true来实现
				- 消息持久化：发送消息时设置MessageProperties的deliveryMode属性来实现，
					deliveryMode属性的值为2时表示消息需要持久化，值为1时表示消息不需要持久化
		(3)针对消费者： 
			消费者开启手动ACK：
				- channel.basicAck(message.getMessageProperties().getDeliveryTag(), false) 
					参数1：deliveryTag消息唯一标识  参数2：是否批量确认
				— channel.basicReject()方法拒绝消息并重新入队，以便重复处理
					参数1：deliveryTag消息唯一标识  参数2：是否重新入队

2.RabbitMq解决消息重复消费的问题
	(1)消息去重（一般用这个，redis横向扩展也方便，支持高并发访问）
		对消费端已经消费过的数据进行标记，消费完再redis缓存消息的唯一标识和过期时间1天，下次再消费的时候判断缓存中是否存在就行了
	(2)消费端做幂等
		a.数据库唯一约束：建一张表保存消息，并对消息的唯一标识创建唯一索引，消费成功的时候进行入库，下一次相同消息进
	来先根据唯一标识去数据库查询，存在则直接ack

3.RabbitMq如何解决消息堆积的问题
	1.增加消费者数量，对某个队列进行消费

```

## 2.利用模版方法模式、工厂模式重构创建订单接口，使其兼容各种类型的订单创建，提高了代码的可塑性

    1.利用模版方法模式+工厂模式重构创建订单接口


    原因：在之前的时候每多一种订单类型，代码有需要多一个
    if_else,他们可能只是计算规则不一样，以及获取的
    折扣优惠不一样，但是每次加一个订单都需要改原来
    的代码，不利于扩展和维护，甚至会出问题。
    
    解决：创建一个抽象的订单公共类，把所有订单的共有逻辑写成模版固定好
    然后将他们不同的方法抽取成抽象的方法，比如计算定的优惠、获取订单的
    折扣抽取成两个抽象的接口，然后然相应的订单类型去实现，比如vip订单
    就集成这个公共的模版实现他的优惠计算和折扣计算，普通订单一样
    	然后再利用工厂模式提供一个根据订单类型创建订单的接口，对外部提
    供工厂接口，不暴露我们底层的实现，这样对扩展开发，修改关闭了

## 3.使用Redis设计订单的购物车功能

    Hash key:购物车前缀+useId  key：productId  value:商品信息

## 4.使用Redisson分布式锁解决秒杀场景下的并发问题

```
1.在后台配置需要进行秒杀的商品，并关联他的场次，设置他的开始时间和结束时间、限购数量、秒杀总数据、秒杀价格等信息
2.利用定时任务在晚上凌晨上架近3天需要进行秒杀的商品，并将其活动信息和活动关联的商品信息缓存到redis当中，
如果缓存已经存在那就不用上传这个信息了，包括商品信息和场次信息缓存，如果修改了场次或删除缓存数据
(1)活动信息：
	Key:活动信息前缀+开始时间_结束时间，Value:活动场次Id+商品Id
(2)商品信息：
	Key:秒杀商品前缀 Key:活动场次Id+商品Id Value:商品的Json信息（包括商品的开始时间、结束时间、价格、随机码）
(3)库存信号量信息：
	Key:信号量前缀+随机码  Value:可以秒杀的总数量
3.秒杀没开始时：
	(1)缓存活动信息、商品信息、商品库存信号量信息
	(2)秒杀商品展示：根据缓存的活动信息查询，查出对应活动场次的所有商品进行列表的展示，如果商品还没有到达秒杀时间
随机码返回空，当商品在秒杀时间的时候才去reids获取相应商品的随机码
4.秒杀开始时（三个参数：KilId = "活动场次Id和商品Id",key = "随机码", num = "购买数量" ）：
	(1)判断当前登录人是否为渠道商
	(2)根据kildId，查询缓存中的商品数据
	(3)判断当前商品是否还在处于秒杀时间内
	(4)判断随机码和kildId是否与缓存的一致
	(5)判断购买数量有没有超过后台配置的值
	(6)判断此人是否购买过了，如果没有就利用setIfAbsent
		设置Key:渠道Id+kildId Value:商品数量 过期时间：活动结束时间-活动开始时间 时间：毫秒
	(7)根据库存信号量前缀+随机码，获取当前商品的库存信号量，执行semaphore.tryAcquire（指定购买数量），
		对库存的原子扣减，能减成功返回True则说明库存足够
	(8)然后快速下单，给Mq发送消息

Mq配置：
	采用的是路由模式，交换机xaas.order.exchange,路由键：xaas.order.seckill,队列：xaas.order.seckill.queue

					

```

## 5.解决过线上OOM内存溢出的问题

    话术：
    	有的，之前在线上环境的时候出现过类似的内存溢出的问题，导致环境不可用，然后我也排查过一些简单的
    问题吧，这个还是要具体问题具体分析的，当时出现内存溢出的时候我是这样处理的：
    	1.先说排查
    	2.根据排查的问题进行分析
    	3.根据具体的命令设置JVM参数信息
    
    	1、利用top命令对当前服务器内存有个大致了解,可以看到占用内存比较大的进程
    	2、利用ps命令查看服务pid进程号   ps -ef |grep java
    	3、利用jstat查看虚拟机gc情况
     		jstat -gcutil  17561  1000 10  //一秒钟采集一次，一共采集10次
    	4、分析youngGC和FullGC次数
    	5、通过进程号利用jmap生成dump文件
    		jmap -dump:format=b,file=heap.prof  17561 
    	6.利用jvisualvm打开dump文件进行分析【点击页面上的装入dump文件】，就可以看到对象的实例数，大小
    	等信息，在定位到那个类占用的内存更大，在根据代码进行分析定位具体类进行优化
    
    	优化方式：
    		减少实例的创建和销毁，复用已有的实例。
    		优化数据结构和算法，减少内存占用。
    		注意对象的引用关系，避免出现死循环和内存泄漏等
    
    	一般的JVM优化通过设置堆内存的大小、新生代、老年代的大小、使用哪种垃圾回收器，降低STW现象出现的频率，
    	从而是是系统具备高可用、低停顿的特点

## 6.利用Explain优化过数据库的索引

    这边做到的索引优化主要包括这些吧
    1.项目初期创建的索引比较多比较杂，我负责将这些索引进行统计，删除部分不需要的索引，建立1~2个符合大部分场景的复合索引
    索引也是需要占用一定内存的，索引多不一定高效
    2.利用Explain将索引达到type = ref或者range级别，
    3.不要用select*,要尽量指定字段,指定字段的情况下有时候是可以走覆盖索引的，这样就减少了回表的次数，效率也会更高
    【聚簇索引也就是我们的主键索引，叶子节点保留了具体的行数据，非聚簇索引又叫辅助索引，如果说辅助索引上面的数据符合查询的数据，那么就不用回表到主键索引上面去拿全部数据了，这也叫做覆盖索引】

## 7.利用DDD领域驱动思想对代码进行拆分，使用充血模型对实体进行管理

    	DDD领域驱动模型是一种偏向于业务划分、尽量是我们的代码具备”高内聚，低耦合“的一种思想，他以更加简洁的方式让我们
    看清楚复杂的项目结构并且使我们的项目不会老的太快（便于扩展）。【一种什么思想，解决什么问题】
    	传统的微服务主要针对应用服务的层次，利用MVC三层结构，他主要看中的是逻辑的实现，所有逻辑上手就写，代码耦合较高，
    而DDD主要针对于业务进行进一步的拆分，不关心具体逻辑实现，将具体的实现逻辑都抽取了出来封装成接口，简化了service层代
    码，也降低了业务之间的耦合，我们也不需要关注代码的具体实现，如果后续需要改动、或者扩展我们在实现的接口里面改、或者新
    增就好了。【传统和DDD简单对比】
    	贫血模式【MVC】：贫血模型看不出这个实体具体做什么业务的,而且加字段会影响多个使用到这个实体类的service方法
    	充血模式【DDD】：将引起实体变化的业务方法都写到实体里面，那么以后我看这个实体就知道他参与了什么业务了，而且加字段，
    只会影响实体里面对应使用到该字段的业务方法，充血模型解决了贫血失忆症
    	实体变化频繁用充血模型去实现，变化不大用贫血模型【贫血模式和充血模式区别】
    	仓库：用来操作数据库层的逻辑，这样上层业务就不用管是怎么处理了 只关注业务就行（仓库和工厂就是持久化数据操作分库分
    表或者mq异步处理数据  都是在仓库处理）【利用仓库来隔离操作数据库的操作】
    	防腐层：调用第三方的时候，将这一类的方法抽取到一个接口里面，这里面定义我们自己的返回操作，隔离外部服务，摆脱技术框
    架限制【意义：防腐层认为外面都是不好的，自己的都是好的】
    
    	领域模型：购物车和商品项被建模为领域模型中的实体类，领域服务则被封装在购物车服务类中。
    	聚合：购物车和商品项被组合成一个聚合，购物车是聚合的根实体，在购物车实体类中封装了添加、删除商品项和计算总价格等操作。
    	领域事件：在这个示例中，没有使用领域事件，不过可以通过引入领域事件来实现领域模型之间的解耦和通信

## 8.Transactional解决本地事务问题

```
```

## 9.对ELK的理解

```

	ELK 是一个开源的日志管理平台，是由三个开源软件组成的组合，分别是 Elasticsearch、Logstash 和 Kibana。
这三个软件的首字母缩写组成了 ELK。
	Elasticsearch 是一个基于 Lucene 的分布式搜索引擎，用于存储、搜索和分析大量的结构化和非结构化数据。
	Logstash 是一个用于收集、过滤、转换和输出日志的数据管道工具，它可以从多个来源（如文件、网络、数据库等）
收集日志数据，并对数据进行处理和转换。
	Kibana 是一个用于可视化和分析 Elasticsearch 数据的 Web 应用程序，它提供了强大的图表和图形界面，可以帮
助用户更直观地理解和分析日志数据。
	ELK 可以用于实时的日志分析、数据可视化和告警，可以帮助用户更快速地定位和解决问题，同时也可以用于监控和分
析系统性能和应用程序运行状态等。
	ELK 的基本工作流程是：Logstash 从数据源收集和处理数据，将处理后的数据存储到 Elasticsearch 中，Kibana 
对 Elasticsearch 中的数据进行可视化和分析。ELK 还支持通过插件和扩展来扩展功能，如 Beats 插件可以用于从
主机和容器中收集数据，Logstash 的插件可以用于支持更多的数据源和数据格式等。
```

# 源码理解

1.Spring源码

    底层是工厂模式+配置文件+反射实现的

2.SpringMvc源码

    1.前端控制器（中转站）接受请求，但不会处理请求，他会把请求路径交给处理器映射器，
    2.在项目启动的时候，处理器映射器把所有的Controller创建好（路径和方法对象保存到Map），根据路径找到指定方法将方法封装成handle(处理器)，交给dispatcherServlet，
    3.dispatcherServlet让适配器去处理处理器，统一将处理器里面的东西（执行里面的方法），封装成ModelAndView对象（可以有视图+其他数据），在返回给dispatcherServlet（底层是适配器模式）
    4.dispatcherServlet将ModelAndView对象交给视图解析器 ，视图解析器将逻辑视图-->物理视图
    最后交给前端控制器，他帮你响应页面进行页面渲染

3.Mybatis源码

    (1)执行原理：
    	mybatis会将各种要执行的statement<select、update、delete、insert>通过配置文件配置起来，
    在执行CRUD的时候通过namespace+id找到我们要执行的statement，最终运行statement封装结果集
    (2)缓存机制：
     	mybatis一级缓存数据是缓存在sqlsession中，通过同一个sqlsession对象去获取数据的时候，第一次从
    数据库中获取的数据会缓存到sqlsession中，下次再获取相同数据的时候，直接从sqlsession中获取（事务级别的缓存）
    	- 1.一级缓存清空清空(会话级别的缓存)   数据保存在sqlSession中
                1.执行了增删改一级缓存的数据自动清空
                2.直接关闭sqlSession或者提交事物
                3.手动调用sqlSession.clearCache();
    
    	mybatis的二级缓存数据是缓存在SqlSessionFactory(一个项目只会创建一个SqlSessionFactory对象)
    	- 2.（mapper级别的缓存） 	
    		将不同的缓存放到各自的缓存块里面
    		修改数据 会清空当前mapperd的缓存

4.AQS源码

    与AQS有关的类【底层都是AQS做支撑】
        ReentrantLock
        
        CountDownLatch
        ReentrantReadWriteLock
        Semaphore
        AQS相关框架图
    
    (1)tryAcquire主要就是尝试抢占锁资源，抢不到addwaiter就负责将线程进行排队
    	流程
        1.获取当前state,判断当前资源是否被占用
        2.未被占用（资源state=0），则CAS判断尝试抢占资源（尝试把state由0-->1）,成功则设置当前线程独占资源并返回true
        3.判断当前线程是否是独占资源的线程，是则放回true,否则返回false
    
    (2)addWaiter主要赋值创建节点入队的操作
    	流程
        1.判断尾指针上面有元素，有元素CAS设置尾节点
        2.如果尾指针没有元素，则调用enq（入队操作）方法
        3.【enq方法】：enq 死循环 判断尾结点是否为空（只有第一次会为空），为空则创建一个虚节点初始化该节点信息
    		thread=null;waitStatus=0。(尾节点插入Node元素)
        4.尾结点不为空，将当前节点prev指向上一节点，尾结点指向当前节点，上一节点的next指向该节点
    
    (3)cquireQueued将没有强抢到资源的线程排好序阻塞在队列中，等待资源的释放,调用unlock唤醒之后重新抢占资源
    	流程
        1.判断尾指针上面有元素，有元素CAS设置尾节点
        2.如果尾指针没有元素，则调用enq（入队操作）方法
        3.【enq方法】：enq 死循环 判断尾结点是否为空（只有第一次会为空），为空则创建一个虚节点初始化该节点信息
    		thread=null;waitStatus=0。(尾节点插入Node元素)
        4.尾结点不为空，将当前节点prev指向上一节点，尾结点指向当前节点，上一节点的next指向该节点

5.unsafe类源码

```
```

6.ThreadLocal源码

```
```
